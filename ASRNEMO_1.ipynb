{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PERFECTINGASRNEMO.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "O1wVmZYs2Nwt",
        "yxjP7a0MBBAo"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNE6FKEvuWb7s/AxKs21ZJx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a7609f4a75544f2c96c456700668ea69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_738d9043f3e64dcca4e23af61ed60c35",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9e69e4e5c7414431aad65deb74f63e30",
              "IPY_MODEL_864f45f2c3d247dc9650e2a80989dd56"
            ]
          }
        },
        "738d9043f3e64dcca4e23af61ed60c35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9e69e4e5c7414431aad65deb74f63e30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_51f1154fe0564403b3fe0db8fab82378",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 442,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 442,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6ec3ee9761cd48989c594d4b335289c8"
          }
        },
        "864f45f2c3d247dc9650e2a80989dd56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9fcd3920bccf4746b4704db63f5ab29d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 442/442 [00:00&lt;00:00, 4.20kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_05877235386e4506b0832210083e4612"
          }
        },
        "51f1154fe0564403b3fe0db8fab82378": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6ec3ee9761cd48989c594d4b335289c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9fcd3920bccf4746b4704db63f5ab29d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "05877235386e4506b0832210083e4612": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a7071bac962c43179cbe5291e280f390": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_675e36966c4248b988329ae94d65c3ad",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ba135124a10b4ca1a16d8afdbd2d3672",
              "IPY_MODEL_8572780402c344f49271498ec3c92af8"
            ]
          }
        },
        "675e36966c4248b988329ae94d65c3ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba135124a10b4ca1a16d8afdbd2d3672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0d645a28b04d4ff690522eae39bd86ea",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a30700a464f4849a84c8df97ec7f6c1"
          }
        },
        "8572780402c344f49271498ec3c92af8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1ba54c13c3ac4ed0ac66bad29164470a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 796kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0c8cae77bc3a4d63b1e7dc5dead7a51b"
          }
        },
        "0d645a28b04d4ff690522eae39bd86ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a30700a464f4849a84c8df97ec7f6c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1ba54c13c3ac4ed0ac66bad29164470a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0c8cae77bc3a4d63b1e7dc5dead7a51b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "720c8582e45b47d4a90324b1a1e30c55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3216efbdaa69491aa9b3978b7be6e793",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f00b7702d72a4eb4b75be2bc5c97dc19",
              "IPY_MODEL_ded88a084ab2493a9fcd3d9f312a99e7"
            ]
          }
        },
        "3216efbdaa69491aa9b3978b7be6e793": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f00b7702d72a4eb4b75be2bc5c97dc19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_634b21e21dab4b07afa34c10ebcaf945",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 267967963,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 267967963,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_55639e3d710c4800b1495f9d4b3111ef"
          }
        },
        "ded88a084ab2493a9fcd3d9f312a99e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e37c64609fe447f7be64519ef4886f7a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 268M/268M [00:10&lt;00:00, 24.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_81477c8f82754eda8e4b1f19366ba7dc"
          }
        },
        "634b21e21dab4b07afa34c10ebcaf945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "55639e3d710c4800b1495f9d4b3111ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e37c64609fe447f7be64519ef4886f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "81477c8f82754eda8e4b1f19366ba7dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreschen33/Yumera/blob/main/ASRNEMO_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1wVmZYs2Nwt"
      },
      "source": [
        "# CONNECTING TO DRIVE AND BUILD THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAztoCjSVOdR"
      },
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKKvCJbEU5jy"
      },
      "source": [
        "nb_path ='/content/notebooks'\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0,nb_path)\n",
        "\"\"\"place the nb_path in the middle section of install pip like examples : \n",
        "    pip install --target==$nb_path blablabla\"\"\"\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY8L1V8nUue9"
      },
      "source": [
        "https://stackoverflow.com/questions/55253498/how-do-i-install-a-library-permanently-in-colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FX8A4xF159W"
      },
      "source": [
        "# INSTALLING DEPENDENCIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrgyywQo71tH"
      },
      "source": [
        "## Install dependencies\n",
        "!pip install wget\n",
        "!apt-get install libsndfile1 ffmpeg\n",
        "!pip install unidecode\n",
        "!pip install matplotlib>=3.3.2\n",
        "!pip install torchaudio\n",
        "## Install NeMo\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[all]\n",
        "\n",
        "\n",
        "## Grab the config we'll use in this example\n",
        "!mkdir configs\n",
        "!wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/asr/conf/config.yaml\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxU3fNOW88rV"
      },
      "source": [
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[all]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbowGXPGYm-p"
      },
      "source": [
        "pip install imgaug==0.2.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bacto5TlYqdt"
      },
      "source": [
        "pip install Sphinx==2.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_T99S9YYuqf"
      },
      "source": [
        "pip install folium==0.2.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuoVsv8RwZfw"
      },
      "source": [
        "pip install Sphinx==2.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzv9bD46ykRG"
      },
      "source": [
        "pip install urllib3==1.25.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kmTl6L7cpJ5"
      },
      "source": [
        "## Install dependencies\n",
        "!pip install wget\n",
        "!apt-get install libsndfile1 ffmpeg\n",
        "!pip install unidecode\n",
        "!pip install matplotlib>=3.3.2\n",
        "!pip install torchaudio\n",
        "## Install NeMo\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[all]\n",
        "\n",
        "## Grab the config we'll use in this example\n",
        "!mkdir configs\n",
        "!wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/asr/conf/config.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZROssjbderY"
      },
      "source": [
        "!pip install ffmpeg-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdTDH53m1K_2"
      },
      "source": [
        "# IMPORTING NECESSARY MODULES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsDPiBOV-c-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7034425-6b91-4447-c8b8-68ff0ee98ec0"
      },
      "source": [
        "import nemo\n",
        "import nemo.collections.asr as nemo_asr\n",
        "import nemo.collections.nlp as nemo_nlp\n",
        "import nemo.collections.tts as nemo_tts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[NeMo W 2021-01-14 03:55:36 experimental:28] Module <class 'nemo.collections.asr.losses.ctc.CTCLoss'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[NeMo W 2021-01-14 03:55:40 experimental:28] Module nemo.collections.asr.data.audio_to_text.AudioToCharDataset is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
            "[NeMo W 2021-01-14 03:55:40 experimental:28] Module nemo.collections.asr.data.audio_to_text.AudioToBPEDataset is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
            "[NeMo W 2021-01-14 03:55:40 experimental:28] Module nemo.collections.asr.data.audio_to_text.AudioLabelDataset is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
            "[NeMo W 2021-01-14 03:55:40 experimental:28] Module nemo.collections.asr.data.audio_to_text.TarredAudioToTextDataset is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
            "[NeMo W 2021-01-14 03:55:40 experimental:28] Module nemo.collections.asr.data.audio_to_text.TarredAudioToCharDataset is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
            "[NeMo W 2021-01-14 03:55:40 experimental:28] Module nemo.collections.asr.data.audio_to_text.TarredAudioToBPEDataset is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
            "[NeMo W 2021-01-14 03:55:40 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text_dali.AudioToCharDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
            "################################################################################\n",
            "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
            "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
            "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
            "################################################################################\n",
            "\n",
            "[NeMo W 2021-01-14 03:55:40 nemo_logging:349] /usr/local/lib/python3.6/dist-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
            "      '\"sox\" backend is being deprecated. '\n",
            "    \n",
            "[NeMo W 2021-01-14 03:55:41 nemo_logging:349] /usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
            "      return torch._C._cuda_getDeviceCount() > 0\n",
            "    \n",
            "[NeMo W 2021-01-14 03:55:42 experimental:28] Module <class 'nemo.collections.nlp.modules.common.megatron.megatron_bert.MegatronBertEncoder'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "WARNING: APEX is not installed, multi_tensor_applier will not be available.\n",
            "WARNING: APEX is not installed, using torch.nn.LayerNorm instead of apex.normalization.FusedLayerNorm!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0_T7qxv-ET2"
      },
      "source": [
        "# We'll use this to listen to audio\n",
        "import IPython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxjP7a0MBBAo"
      },
      "source": [
        "# VOICE CODE MODELS\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4AWZobddX_w"
      },
      "source": [
        "https://colab.research.google.com/drive/1Z6VIRZ_sX314hyev3Gm5gBqvm1wQVo-a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVC-6GUNbH4l"
      },
      "source": [
        "\"\"\"\n",
        "To write this piece of code I took inspiration/code from a lot of places.\n",
        "It was late night, so I'm not sure how much I created or just copied o.O\n",
        "Here are some of the possible references:\n",
        "https://blog.addpipe.com/recording-audio-in-the-browser-using-pure-html5-and-minimal-javascript/\n",
        "https://stackoverflow.com/a/18650249\n",
        "https://hacks.mozilla.org/2014/06/easy-audio-capture-with-the-mediarecorder-api/\n",
        "https://air.ghost.io/recording-to-an-audio-file-using-html5-and-js/\n",
        "https://stackoverflow.com/a/49019356\n",
        "\"\"\"\n",
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "//my_p.appendChild(my_btn);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "    //mimeType : 'audio/webm;codecs=pcm'\n",
        "  };            \n",
        "  //recorder = new MediaRecorder(stream, options);\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {            \n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data); \n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "      //console.log(\"Inside FileReader:\" + base64data);\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  //console.log(\"Inside data:\" + base64data)\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "      \n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def get_audio():\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  \n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('pipe:1', format='wav')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "  \n",
        "  riff_chunk_size = len(output) - 8\n",
        "  # Break up the chunk size into four bytes, held in b.\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "  sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "  return audio, sr\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn5F3S2YBEyY"
      },
      "source": [
        "# BUILDING THE SPEECH RECOGNITION MODELS\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65_vncPG2qg-"
      },
      "source": [
        "IMPORTING SPECTROGRAM QUARTZNET AND OTHERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FhBujhKAMTX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844,
          "referenced_widgets": [
            "a7609f4a75544f2c96c456700668ea69",
            "738d9043f3e64dcca4e23af61ed60c35",
            "9e69e4e5c7414431aad65deb74f63e30",
            "864f45f2c3d247dc9650e2a80989dd56",
            "51f1154fe0564403b3fe0db8fab82378",
            "6ec3ee9761cd48989c594d4b335289c8",
            "9fcd3920bccf4746b4704db63f5ab29d",
            "05877235386e4506b0832210083e4612",
            "a7071bac962c43179cbe5291e280f390",
            "675e36966c4248b988329ae94d65c3ad",
            "ba135124a10b4ca1a16d8afdbd2d3672",
            "8572780402c344f49271498ec3c92af8",
            "0d645a28b04d4ff690522eae39bd86ea",
            "4a30700a464f4849a84c8df97ec7f6c1",
            "1ba54c13c3ac4ed0ac66bad29164470a",
            "0c8cae77bc3a4d63b1e7dc5dead7a51b",
            "720c8582e45b47d4a90324b1a1e30c55",
            "3216efbdaa69491aa9b3978b7be6e793",
            "f00b7702d72a4eb4b75be2bc5c97dc19",
            "ded88a084ab2493a9fcd3d9f312a99e7",
            "634b21e21dab4b07afa34c10ebcaf945",
            "55639e3d710c4800b1495f9d4b3111ef",
            "e37c64609fe447f7be64519ef4886f7a",
            "81477c8f82754eda8e4b1f19366ba7dc"
          ]
        },
        "outputId": "d946b4e1-ccd5-49f9-8228-5686afd46637"
      },
      "source": [
        "# Automatic Speech Recognition Models\n",
        "quartznet = nemo_asr.models.EncDecCTCModel.from_pretrained(model_name=\"QuartzNet15x5Base-En\",strict=False)\n",
        "# Punctuation and capitalization model\n",
        "punctuation = nemo_nlp.models.PunctuationCapitalizationModel.from_pretrained(model_name='Punctuation_Capitalization_with_DistilBERT', strict = False)\n",
        "# Spectrogram generator which takes text as an input and produces spectrogram\n",
        "spectrogram_generator = nemo_tts.models.Tacotron2Model.from_pretrained(model_name=\"Tacotron2-22050Hz\", strict = False)\n",
        "# Vocoder model which takes spectrogram and produces actual audio\n",
        "vocoder = nemo_tts.models.WaveGlowModel.from_pretrained(model_name=\"WaveGlow-22050Hz\", strict =False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[NeMo I 2021-01-14 03:55:42 cloud:66] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/QuartzNet15x5Base-En.nemo to /root/.cache/torch/NeMo/NeMo_1.0.0b4/QuartzNet15x5Base-En/00869f9c89b8393ca3de640e0c536bd2/QuartzNet15x5Base-En.nemo\n",
            "[NeMo I 2021-01-14 03:55:43 common:423] Instantiating model from pre-trained checkpoint\n",
            "[NeMo I 2021-01-14 03:55:44 features:236] PADDING: 16\n",
            "[NeMo I 2021-01-14 03:55:44 features:252] STFT using torch\n",
            "[NeMo I 2021-01-14 03:55:44 modelPT:402] Model EncDecCTCModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.0.0b4/QuartzNet15x5Base-En/00869f9c89b8393ca3de640e0c536bd2/QuartzNet15x5Base-En.nemo.\n",
            "[NeMo I 2021-01-14 03:55:44 cloud:66] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemonlpmodels/versions/1.0.0a5/files/Punctuation_Capitalization_with_DistilBERT.nemo to /root/.cache/torch/NeMo/NeMo_1.0.0b4/Punctuation_Capitalization_with_DistilBERT/6906a3705a7989aa9995aed680fb5d22/Punctuation_Capitalization_with_DistilBERT.nemo\n",
            "[NeMo I 2021-01-14 03:55:49 common:423] Instantiating model from pre-trained checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7609f4a75544f2c96c456700668ea69",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7071bac962c43179cbe5291e280f390",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using bos_token, but it is not set yet.\n",
            "Using eos_token, but it is not set yet.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[NeMo W 2021-01-14 03:55:53 modelPT:146] Please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    text_file: text_train.txt\n",
            "    labels_file: labels_train.txt\n",
            "    shuffle: true\n",
            "    num_samples: -1\n",
            "    batch_size: 64\n",
            "    \n",
            "[NeMo W 2021-01-14 03:55:53 modelPT:153] Please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    ds_item: null\n",
            "    text_file: text_dev.txt\n",
            "    labels_file: labels_dev.txt\n",
            "    shuffle: false\n",
            "    num_samples: -1\n",
            "    batch_size: 64\n",
            "    \n",
            "[NeMo W 2021-01-14 03:55:53 modelPT:1175] World size can only be set by PyTorch Lightning Trainer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "720c8582e45b47d4a90324b1a1e30c55",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[NeMo I 2021-01-14 03:56:07 modelPT:402] Model PunctuationCapitalizationModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.0.0b4/Punctuation_Capitalization_with_DistilBERT/6906a3705a7989aa9995aed680fb5d22/Punctuation_Capitalization_with_DistilBERT.nemo.\n",
            "[NeMo I 2021-01-14 03:56:07 cloud:66] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemottsmodels/versions/1.0.0a5/files/Tacotron2-22050Hz.nemo to /root/.cache/torch/NeMo/NeMo_1.0.0b4/Tacotron2-22050Hz/59f6603364e11524c03c95920e7afaa0/Tacotron2-22050Hz.nemo\n",
            "[NeMo I 2021-01-14 03:56:10 common:423] Instantiating model from pre-trained checkpoint\n",
            "[NeMo I 2021-01-14 03:56:11 features:236] PADDING: 16\n",
            "[NeMo I 2021-01-14 03:56:11 features:245] STFT using conv\n",
            "[NeMo I 2021-01-14 03:56:13 modelPT:402] Model Tacotron2Model was successfully restored from /root/.cache/torch/NeMo/NeMo_1.0.0b4/Tacotron2-22050Hz/59f6603364e11524c03c95920e7afaa0/Tacotron2-22050Hz.nemo.\n",
            "[NeMo I 2021-01-14 03:56:13 cloud:66] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemottsmodels/versions/1.0.0a5/files/WaveGlow-22050Hz.nemo to /root/.cache/torch/NeMo/NeMo_1.0.0b4/WaveGlow-22050Hz/63a329dc3e8b44ec2e07cd4209eeab2a/WaveGlow-22050Hz.nemo\n",
            "[NeMo I 2021-01-14 03:56:33 common:423] Instantiating model from pre-trained checkpoint\n",
            "[NeMo I 2021-01-14 03:56:42 features:236] PADDING: 16\n",
            "[NeMo I 2021-01-14 03:56:42 features:245] STFT using conv\n",
            "[NeMo I 2021-01-14 03:56:48 modelPT:402] Model WaveGlowModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.0.0b4/WaveGlow-22050Hz/63a329dc3e8b44ec2e07cd4209eeab2a/WaveGlow-22050Hz.nemo.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPxSAQDmBWfH"
      },
      "source": [
        "AUDIO RECORDING "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "updq5mxUxVZA"
      },
      "source": [
        "audio, sr = get_audio()\n",
        "import scipy \n",
        "scipy.io.wavfile.write('a.wav', sr, audio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF2O-ycVw7qI"
      },
      "source": [
        "filesx = \"/content/a.wav\"\r\n",
        "files = [filesx]\r\n",
        "IPython.display.Audio('a.wav')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZDYTv4lCvSo"
      },
      "source": [
        "BUILD SPEECH RECOGNITION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVyO_t3YCEt9"
      },
      "source": [
        "# SPEECH RECOGNITION TRANSCRIPTING\n",
        "for fname, transcription in zip(files, quartznet.transcribe(paths2audio_files=files)):\n",
        "  raw_text = transcription"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29hCGrYbBacf"
      },
      "source": [
        "BUILD PUNCTUATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK65gxaM7u1Y"
      },
      "source": [
        "# Add capitalization and punctuation\n",
        "res = punctuation.add_punctuation_capitalization(queries=[raw_text])\n",
        "text = res[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHxP1S25Bhee"
      },
      "source": [
        "BUILD SPECTOGRAM AND VOCODER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSTUqEI39dJU"
      },
      "source": [
        "# A helper function which combines Tacotron2 and WaveGlow to go directly from \n",
        "# text to audio\n",
        "def text_to_audio(text):\n",
        "  parsed = spectrogram_generator.parse(text)\n",
        "  spectrogram = spectrogram_generator.generate_spectrogram(tokens=parsed)\n",
        "  audio = vocoder.convert_spectrogram_to_audio(spec=spectrogram)\n",
        "  return audio.to('cpu').numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxKgrVyLBkfF"
      },
      "source": [
        "OUTPUT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIDDDUoO-SOv"
      },
      "source": [
        "# Without punctuation\n",
        "print('-------> Without Punctuation')\n",
        "print(f\"Audio in {fname} was recognized as: {raw_text}\")\n",
        "IPython.display.Audio(text_to_audio(raw_text), rate=27000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaxBq9CR9lss"
      },
      "source": [
        "# with Punctuation\n",
        "print('-------> With Punctuation')\n",
        "print(f'\\nRaw recognized text: {raw_text}. \\nText with capitalization and punctuation: {text}')\n",
        "IPython.display.Audio(text_to_audio(text), rate = 27000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hL5NN9xpXRY"
      },
      "source": [
        "# BUILD SPEECH COMMAND MODELS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onnIn6Obz5kA"
      },
      "source": [
        "https://github.com/NVIDIA/NeMo/blob/main/tutorials/asr/04_Online_Offline_Speech_Commands_Demo.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB2nI7-YtoTb"
      },
      "source": [
        "import librosa\r\n",
        "import numpy as np\r\n",
        "import os, time\r\n",
        "import librosa\r\n",
        "import IPython.display as ipd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "from omegaconf import OmegaConf\r\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahwHTl8MFX0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a4d7a2-b1af-40d1-b1d8-4fcae05e3ab3"
      },
      "source": [
        "mbn_model =nemo_asr.models.EncDecClassificationModel.from_pretrained(model_name='MatchboxNet-3x2x64-v1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[NeMo I 2021-01-13 12:45:08 cloud:66] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/MatchboxNet-3x2x64-v1.nemo to /root/.cache/torch/NeMo/NeMo_1.0.0b4/MatchboxNet-3x2x64-v1/8a0833bb977e90f94738e2c6c3da61f3/MatchboxNet-3x2x64-v1.nemo\n",
            "[NeMo I 2021-01-13 12:45:10 common:423] Instantiating model from pre-trained checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[NeMo W 2021-01-13 12:45:10 modelPT:146] Please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: null\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - bed\n",
            "    - bird\n",
            "    - cat\n",
            "    - dog\n",
            "    - down\n",
            "    - eight\n",
            "    - five\n",
            "    - four\n",
            "    - go\n",
            "    - happy\n",
            "    - house\n",
            "    - left\n",
            "    - marvin\n",
            "    - nine\n",
            "    - 'no'\n",
            "    - 'off'\n",
            "    - 'on'\n",
            "    - one\n",
            "    - right\n",
            "    - seven\n",
            "    - sheila\n",
            "    - six\n",
            "    - stop\n",
            "    - three\n",
            "    - tree\n",
            "    - two\n",
            "    - up\n",
            "    - wow\n",
            "    - 'yes'\n",
            "    - zero\n",
            "    batch_size: 128\n",
            "    shuffle: true\n",
            "    augmentor:\n",
            "      shift:\n",
            "        prob: 1.0\n",
            "        min_shift_ms: -5.0\n",
            "        max_shift_ms: 5.0\n",
            "      white_noise:\n",
            "        prob: 1.0\n",
            "        min_level: -90\n",
            "        max_level: -46\n",
            "    \n",
            "[NeMo W 2021-01-13 12:45:10 modelPT:153] Please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath: null\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - bed\n",
            "    - bird\n",
            "    - cat\n",
            "    - dog\n",
            "    - down\n",
            "    - eight\n",
            "    - five\n",
            "    - four\n",
            "    - go\n",
            "    - happy\n",
            "    - house\n",
            "    - left\n",
            "    - marvin\n",
            "    - nine\n",
            "    - 'no'\n",
            "    - 'off'\n",
            "    - 'on'\n",
            "    - one\n",
            "    - right\n",
            "    - seven\n",
            "    - sheila\n",
            "    - six\n",
            "    - stop\n",
            "    - three\n",
            "    - tree\n",
            "    - two\n",
            "    - up\n",
            "    - wow\n",
            "    - 'yes'\n",
            "    - zero\n",
            "    batch_size: 128\n",
            "    shuffle: false\n",
            "    val_loss_idx: 0\n",
            "    \n",
            "[NeMo W 2021-01-13 12:45:10 modelPT:160] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath:\n",
            "    - /home/smajumdar/PycharmProjects/nemo-eval/nemo_eval/speech_commands/v1_validation_manifest.json\n",
            "    - /home/smajumdar/PycharmProjects/nemo-eval/nemo_eval/speech_commands/v1_test_manifest.json\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - bed\n",
            "    - bird\n",
            "    - cat\n",
            "    - dog\n",
            "    - down\n",
            "    - eight\n",
            "    - five\n",
            "    - four\n",
            "    - go\n",
            "    - happy\n",
            "    - house\n",
            "    - left\n",
            "    - marvin\n",
            "    - nine\n",
            "    - 'no'\n",
            "    - 'off'\n",
            "    - 'on'\n",
            "    - one\n",
            "    - right\n",
            "    - seven\n",
            "    - sheila\n",
            "    - six\n",
            "    - stop\n",
            "    - three\n",
            "    - tree\n",
            "    - two\n",
            "    - up\n",
            "    - wow\n",
            "    - 'yes'\n",
            "    - zero\n",
            "    batch_size: 128\n",
            "    shuffle: false\n",
            "    test_loss_idx: 0\n",
            "    \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[NeMo I 2021-01-13 12:45:10 modelPT:402] Model EncDecClassificationModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.0.0b4/MatchboxNet-3x2x64-v1/8a0833bb977e90f94738e2c6c3da61f3/MatchboxNet-3x2x64-v1.nemo.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuO8ZFq_iI-k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29bba288-ced2-410f-9469-5a53f404df26"
      },
      "source": [
        "vad_model = nemo_asr.models.EncDecClassificationModel.from_pretrained('MatchboxNet-VAD-3x2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[NeMo I 2021-01-13 12:45:10 cloud:66] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/MatchboxNet_VAD_3x2.nemo to /root/.cache/torch/NeMo/NeMo_1.0.0b4/MatchboxNet_VAD_3x2/1375f3813383105a24acc75428ec51c4/MatchboxNet_VAD_3x2.nemo\n",
            "[NeMo I 2021-01-13 12:45:12 common:423] Instantiating model from pre-trained checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[NeMo W 2021-01-13 12:45:12 modelPT:146] Please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: /home/fjia/code/manifest64/balanced_background_training_manifest.json,/home/fjia/code/manifest64/balanced_speech_training_manifest.json\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - background\n",
            "    - speech\n",
            "    batch_size: 128\n",
            "    num_workers: 20\n",
            "    shuffle: true\n",
            "    augmentor:\n",
            "      shift:\n",
            "        prob: 0.8\n",
            "        min_shift_ms: -5.0\n",
            "        max_shift_ms: 5.0\n",
            "      white_noise:\n",
            "        prob: 0.8\n",
            "        min_level: -90\n",
            "        max_level: -46\n",
            "    \n",
            "[NeMo W 2021-01-13 12:45:12 modelPT:153] Please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath: /home/fjia/code/manifest64/balanced_background_validation_manifest.json,/home/fjia/code/manifest64/balanced_speech_validation_manifest.json\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - background\n",
            "    - speech\n",
            "    batch_size: 128\n",
            "    shuffle: false\n",
            "    val_loss_idx: 0\n",
            "    num_workers: 20\n",
            "    \n",
            "[NeMo W 2021-01-13 12:45:12 modelPT:160] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath: /home/fjia/code/manifest64/balanced_background_testing_manifest.json,/home/fjia/code/manifest64/balanced_speech_testing_manifest.json\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - background\n",
            "    - speech\n",
            "    batch_size: 128\n",
            "    shuffle: false\n",
            "    test_loss_idx: 0\n",
            "    num_workers: 20\n",
            "    \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[NeMo I 2021-01-13 12:45:13 modelPT:402] Model EncDecClassificationModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.0.0b4/MatchboxNet_VAD_3x2/1375f3813383105a24acc75428ec51c4/MatchboxNet_VAD_3x2.nemo.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVjsARLF7A_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b580cec-fc48-4abe-cbca-45b2cc603ef2"
      },
      "source": [
        "# Preserve a copy of the full config\r\n",
        "vad_cfg = copy.deepcopy(vad_model._cfg)\r\n",
        "mbn_cfg = copy.deepcopy(mbn_model._cfg)\r\n",
        "print(OmegaConf.to_yaml(mbn_cfg))\r\n",
        "\r\n",
        "# Set model to inference mode\r\n",
        "mbn_model.eval();\r\n",
        "vad_model.eval();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "timesteps: 128\n",
            "repeat: 2\n",
            "dropout: 0.0\n",
            "kernel_size_factor: 1.0\n",
            "labels:\n",
            "- bed\n",
            "- bird\n",
            "- cat\n",
            "- dog\n",
            "- down\n",
            "- eight\n",
            "- five\n",
            "- four\n",
            "- go\n",
            "- happy\n",
            "- house\n",
            "- left\n",
            "- marvin\n",
            "- nine\n",
            "- 'no'\n",
            "- 'off'\n",
            "- 'on'\n",
            "- one\n",
            "- right\n",
            "- seven\n",
            "- sheila\n",
            "- six\n",
            "- stop\n",
            "- three\n",
            "- tree\n",
            "- two\n",
            "- up\n",
            "- wow\n",
            "- 'yes'\n",
            "- zero\n",
            "train_ds:\n",
            "  manifest_filepath: null\n",
            "  sample_rate: 16000\n",
            "  labels:\n",
            "  - bed\n",
            "  - bird\n",
            "  - cat\n",
            "  - dog\n",
            "  - down\n",
            "  - eight\n",
            "  - five\n",
            "  - four\n",
            "  - go\n",
            "  - happy\n",
            "  - house\n",
            "  - left\n",
            "  - marvin\n",
            "  - nine\n",
            "  - 'no'\n",
            "  - 'off'\n",
            "  - 'on'\n",
            "  - one\n",
            "  - right\n",
            "  - seven\n",
            "  - sheila\n",
            "  - six\n",
            "  - stop\n",
            "  - three\n",
            "  - tree\n",
            "  - two\n",
            "  - up\n",
            "  - wow\n",
            "  - 'yes'\n",
            "  - zero\n",
            "  batch_size: 128\n",
            "  shuffle: true\n",
            "  augmentor:\n",
            "    shift:\n",
            "      prob: 1.0\n",
            "      min_shift_ms: -5.0\n",
            "      max_shift_ms: 5.0\n",
            "    white_noise:\n",
            "      prob: 1.0\n",
            "      min_level: -90\n",
            "      max_level: -46\n",
            "validation_ds:\n",
            "  manifest_filepath: null\n",
            "  sample_rate: 16000\n",
            "  labels:\n",
            "  - bed\n",
            "  - bird\n",
            "  - cat\n",
            "  - dog\n",
            "  - down\n",
            "  - eight\n",
            "  - five\n",
            "  - four\n",
            "  - go\n",
            "  - happy\n",
            "  - house\n",
            "  - left\n",
            "  - marvin\n",
            "  - nine\n",
            "  - 'no'\n",
            "  - 'off'\n",
            "  - 'on'\n",
            "  - one\n",
            "  - right\n",
            "  - seven\n",
            "  - sheila\n",
            "  - six\n",
            "  - stop\n",
            "  - three\n",
            "  - tree\n",
            "  - two\n",
            "  - up\n",
            "  - wow\n",
            "  - 'yes'\n",
            "  - zero\n",
            "  batch_size: 128\n",
            "  shuffle: false\n",
            "  val_loss_idx: 0\n",
            "test_ds:\n",
            "  manifest_filepath:\n",
            "  - /home/smajumdar/PycharmProjects/nemo-eval/nemo_eval/speech_commands/v1_validation_manifest.json\n",
            "  - /home/smajumdar/PycharmProjects/nemo-eval/nemo_eval/speech_commands/v1_test_manifest.json\n",
            "  sample_rate: 16000\n",
            "  labels:\n",
            "  - bed\n",
            "  - bird\n",
            "  - cat\n",
            "  - dog\n",
            "  - down\n",
            "  - eight\n",
            "  - five\n",
            "  - four\n",
            "  - go\n",
            "  - happy\n",
            "  - house\n",
            "  - left\n",
            "  - marvin\n",
            "  - nine\n",
            "  - 'no'\n",
            "  - 'off'\n",
            "  - 'on'\n",
            "  - one\n",
            "  - right\n",
            "  - seven\n",
            "  - sheila\n",
            "  - six\n",
            "  - stop\n",
            "  - three\n",
            "  - tree\n",
            "  - two\n",
            "  - up\n",
            "  - wow\n",
            "  - 'yes'\n",
            "  - zero\n",
            "  batch_size: 128\n",
            "  shuffle: false\n",
            "  test_loss_idx: 0\n",
            "preprocessor:\n",
            "  _target_: nemo.collections.asr.modules.AudioToMFCCPreprocessor\n",
            "  window_size: 0.025\n",
            "  window_stride: 0.01\n",
            "  window: hann\n",
            "  n_mels: 64\n",
            "  n_mfcc: 64\n",
            "  n_fft: 512\n",
            "spec_augment:\n",
            "  _target_: nemo.collections.asr.modules.SpectrogramAugmentation\n",
            "  freq_masks: 2\n",
            "  time_masks: 2\n",
            "  freq_width: 15\n",
            "  time_width: 25\n",
            "  rect_masks: 5\n",
            "  rect_time: 25\n",
            "  rect_freq: 15\n",
            "crop_or_pad_augment:\n",
            "  _target_: nemo.collections.asr.modules.CropOrPadSpectrogramAugmentation\n",
            "  audio_length: 128\n",
            "encoder:\n",
            "  _target_: nemo.collections.asr.modules.ConvASREncoder\n",
            "  feat_in: 64\n",
            "  activation: relu\n",
            "  conv_mask: true\n",
            "  jasper:\n",
            "  - filters: 128\n",
            "    repeat: 1\n",
            "    kernel:\n",
            "    - 11\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: false\n",
            "    separable: true\n",
            "    kernel_size_factor: 1.0\n",
            "  - filters: 64\n",
            "    repeat: 2\n",
            "    kernel:\n",
            "    - 13\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    kernel_size_factor: 1.0\n",
            "  - filters: 64\n",
            "    repeat: 2\n",
            "    kernel:\n",
            "    - 15\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    kernel_size_factor: 1.0\n",
            "  - filters: 64\n",
            "    repeat: 2\n",
            "    kernel:\n",
            "    - 17\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: true\n",
            "    separable: true\n",
            "    kernel_size_factor: 1.0\n",
            "  - filters: 128\n",
            "    repeat: 1\n",
            "    kernel:\n",
            "    - 29\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 2\n",
            "    dropout: 0.0\n",
            "    residual: false\n",
            "    separable: true\n",
            "    kernel_size_factor: 1.0\n",
            "  - filters: 128\n",
            "    repeat: 1\n",
            "    kernel:\n",
            "    - 1\n",
            "    stride:\n",
            "    - 1\n",
            "    dilation:\n",
            "    - 1\n",
            "    dropout: 0.0\n",
            "    residual: false\n",
            "decoder:\n",
            "  _target_: nemo.collections.asr.modules.ConvASRDecoderClassification\n",
            "  feat_in: 128\n",
            "  num_classes: 30\n",
            "  return_logits: true\n",
            "  pooling_type: avg\n",
            "optim:\n",
            "  name: novograd\n",
            "  lr: 0.05\n",
            "  betas:\n",
            "  - 0.95\n",
            "  - 0.5\n",
            "  weight_decay: 0.001\n",
            "  sched:\n",
            "    name: PolynomialHoldDecayAnnealing\n",
            "    power: 2.0\n",
            "    warmup_ratio: 0.05\n",
            "    hold_ratio: 0.45\n",
            "    min_lr: 0.001\n",
            "    last_epoch: -1\n",
            "target: nemo.collections.asr.models.classification_models.EncDecClassificationModel\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di93ZUAP71Wd"
      },
      "source": [
        "SETTING UP DATA FOR STREAMING INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqE4KiT9s6mV"
      },
      "source": [
        "from nemo.core.classes import IterableDataset\r\n",
        "from nemo.core.neural_types import NeuralType, AudioSignal, LengthsType\r\n",
        "import torch\r\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55nOYHWR7xtO"
      },
      "source": [
        "# simple data layer to pass audio signal\r\n",
        "class AudioDataLayer(IterableDataset):\r\n",
        "    @property\r\n",
        "    def output_types(self):\r\n",
        "        return {\r\n",
        "            'audio_signal': NeuralType(('B', 'T'), AudioSignal(freq=self._sample_rate)),\r\n",
        "            'a_sig_length': NeuralType(tuple('B'), LengthsType()),\r\n",
        "        }\r\n",
        "\r\n",
        "    def __init__(self, sample_rate):\r\n",
        "        super().__init__()\r\n",
        "        self._sample_rate = sample_rate\r\n",
        "        self.output = True\r\n",
        "        \r\n",
        "    def __iter__(self):\r\n",
        "        return self\r\n",
        "    \r\n",
        "    def __next__(self):\r\n",
        "        if not self.output:\r\n",
        "            raise StopIteration\r\n",
        "        self.output = False\r\n",
        "        return torch.as_tensor(self.signal, dtype=torch.float32), \\\r\n",
        "               torch.as_tensor(self.signal_shape, dtype=torch.int64)\r\n",
        "        \r\n",
        "    def set_signal(self, signal):\r\n",
        "        self.signal = signal.astype(np.float32)/32768.\r\n",
        "        self.signal_shape = self.signal.size\r\n",
        "        self.output = True\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8WxcNMJtDdD"
      },
      "source": [
        "data_layer = AudioDataLayer(sample_rate=mbn_cfg.train_ds.sample_rate)\r\n",
        "data_loader = DataLoader(data_layer, batch_size=1, collate_fn=data_layer.collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcHSdkds7gRV"
      },
      "source": [
        "INFERENCE METHOD FOR AUDIO SIGNAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPtjsti_tDgS"
      },
      "source": [
        "def infer_signal(model, signal):\r\n",
        "    data_layer.set_signal(signal)\r\n",
        "    batch = next(iter(data_loader))\r\n",
        "    audio_signal, audio_signal_len = batch\r\n",
        "    audio_signal, audio_signal_len = audio_signal.to(model.device), audio_signal_len.to(model.device)\r\n",
        "    logits = model.forward(input_signal=audio_signal, input_signal_length=audio_signal_len)\r\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBw4rHPhtA9H"
      },
      "source": [
        "# class for streaming frame-based ASR\r\n",
        "# 1) use reset() method to reset FrameASR's state\r\n",
        "# 2) call transcribe(frame) to do ASR on\r\n",
        "#    contiguous signal's frames\r\n",
        "class FrameASR:\r\n",
        "    \r\n",
        "    def __init__(self, model_definition,\r\n",
        "                 frame_len=2, frame_overlap=2.5, \r\n",
        "                 offset=0):\r\n",
        "        '''\r\n",
        "        Args:\r\n",
        "          frame_len (seconds): Frame's duration\r\n",
        "          frame_overlap (seconds): Duration of overlaps before and after current frame.\r\n",
        "          offset: Number of symbols to drop for smooth streaming.\r\n",
        "        '''\r\n",
        "        self.task = model_definition['task']\r\n",
        "        self.vocab = list(model_definition['labels'])\r\n",
        "        \r\n",
        "        self.sr = model_definition['sample_rate']\r\n",
        "        self.frame_len = frame_len\r\n",
        "        self.n_frame_len = int(frame_len * self.sr)\r\n",
        "        self.frame_overlap = frame_overlap\r\n",
        "        self.n_frame_overlap = int(frame_overlap * self.sr)\r\n",
        "        timestep_duration = model_definition['AudioToMFCCPreprocessor']['window_stride']\r\n",
        "        for block in model_definition['JasperEncoder']['jasper']:\r\n",
        "            timestep_duration *= block['stride'][0] ** block['repeat']\r\n",
        "        self.buffer = np.zeros(shape=2*self.n_frame_overlap + self.n_frame_len,\r\n",
        "                               dtype=np.float32)\r\n",
        "        self.offset = offset\r\n",
        "        self.reset()\r\n",
        "        \r\n",
        "    @torch.no_grad()\r\n",
        "    def _decode(self, frame, offset=0):\r\n",
        "        assert len(frame)==self.n_frame_len\r\n",
        "        self.buffer[:-self.n_frame_len] = self.buffer[self.n_frame_len:]\r\n",
        "        self.buffer[-self.n_frame_len:] = frame\r\n",
        "\r\n",
        "        if self.task == 'mbn':\r\n",
        "            logits = infer_signal(mbn_model, self.buffer).to('cpu').numpy()[0]\r\n",
        "            decoded = self._mbn_greedy_decoder(logits, self.vocab)\r\n",
        "            \r\n",
        "        elif self.task == 'vad':\r\n",
        "            logits = infer_signal(vad_model, self.buffer).to('cpu').numpy()[0]\r\n",
        "            decoded = self._vad_greedy_decoder(logits, self.vocab)\r\n",
        "           \r\n",
        "        else:\r\n",
        "            raise(\"Task should either be of mbn or vad!\")\r\n",
        "            \r\n",
        "        return decoded[:len(decoded)-offset]\r\n",
        "    \r\n",
        "    def transcribe(self, frame=None,merge=False):\r\n",
        "        if frame is None:\r\n",
        "            frame = np.zeros(shape=self.n_frame_len, dtype=np.float32)\r\n",
        "        if len(frame) < self.n_frame_len:\r\n",
        "            frame = np.pad(frame, [0, self.n_frame_len - len(frame)], 'constant')\r\n",
        "        unmerged = self._decode(frame, self.offset)\r\n",
        "        return unmerged\r\n",
        "        \r\n",
        "    \r\n",
        "    def reset(self):\r\n",
        "        '''\r\n",
        "        Reset frame_history and decoder's state\r\n",
        "        '''\r\n",
        "        self.buffer=np.zeros(shape=self.buffer.shape, dtype=np.float32)\r\n",
        "        self.mbn_s = []\r\n",
        "        self.vad_s = []\r\n",
        "        \r\n",
        "    @staticmethod\r\n",
        "    def _mbn_greedy_decoder(logits, vocab):\r\n",
        "        mbn_s = []\r\n",
        "        if logits.shape[0]:\r\n",
        "            class_idx = np.argmax(logits)\r\n",
        "            class_label = vocab[class_idx]\r\n",
        "            mbn_s.append(class_label)         \r\n",
        "        return mbn_s\r\n",
        "    \r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def _vad_greedy_decoder(logits, vocab):\r\n",
        "        vad_s = []\r\n",
        "        if logits.shape[0]:\r\n",
        "            probs = torch.softmax(torch.as_tensor(logits), dim=-1)\r\n",
        "            probas, preds = torch.max(probs, dim=-1)\r\n",
        "            vad_s = [preds.item(), str(vocab[preds]), probs[0].item(), probs[1].item(), str(logits)]\r\n",
        "        return vad_s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFM3CR86nQJb"
      },
      "source": [
        "STREAMING INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQFwjrZKu6ac"
      },
      "source": [
        "def listToString(s):  \r\n",
        "    str1 = \" \" \r\n",
        "    return (str1.join(s)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhdNaKvwtX3F"
      },
      "source": [
        "import wave\r\n",
        "\r\n",
        "def offline_inference(wave_file, STEP = 0.25, WINDOW_SIZE = 0.31):\r\n",
        "    \"\"\"\r\n",
        "    Arg:\r\n",
        "        wav_file: wave file to be performed inference on.\r\n",
        "        STEP: infer every STEP seconds \r\n",
        "        WINDOW_SIZE : lenght of audio to be sent to NN.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    FRAME_LEN = STEP \r\n",
        "    CHANNELS = 1 # number of audio channels (expect mono signal)\r\n",
        "    RATE = SAMPLE_RATE # sample rate, 16000 Hz\r\n",
        "   \r\n",
        "    CHUNK_SIZE = int(FRAME_LEN * SAMPLE_RATE)\r\n",
        "    \r\n",
        "    mbn = FrameASR(model_definition = {\r\n",
        "                       'task': 'mbn',\r\n",
        "                       'sample_rate': SAMPLE_RATE,\r\n",
        "                       'AudioToMFCCPreprocessor': mbn_cfg.preprocessor,\r\n",
        "                       'JasperEncoder': mbn_cfg.encoder,\r\n",
        "                       'labels': mbn_cfg.labels\r\n",
        "                   },\r\n",
        "                   frame_len=FRAME_LEN, frame_overlap = (WINDOW_SIZE - FRAME_LEN)/2,\r\n",
        "                   offset=0)\r\n",
        "\r\n",
        "    wf = wave.open(wave_file, 'rb')\r\n",
        "    data = wf.readframes(CHUNK_SIZE)\r\n",
        "\r\n",
        "    while len(data) > 0:\r\n",
        "\r\n",
        "        data = wf.readframes(CHUNK_SIZE)\r\n",
        "        signal = np.frombuffer(data, dtype=np.int16)\r\n",
        "        mbn_result = mbn.transcribe(signal)\r\n",
        "        mbn_result = listToString(mbn_result)\r\n",
        "\r\n",
        "        if len(mbn_result):\r\n",
        "          print(mbn_result)\r\n",
        "\r\n",
        "\r\n",
        "    mbn.reset()\r\n",
        "    return mbn_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMw2mPkhqv52",
        "outputId": "2f3416c8-fbfc-45d0-e657-840c99d9feba"
      },
      "source": [
        "import wave\r\n",
        "import contextlib\r\n",
        "fname = filesx\r\n",
        "with contextlib.closing(wave.open(fname,'r')) as f:\r\n",
        "    frames = f.getnframes()\r\n",
        "    rate = f.getframerate()\r\n",
        "    duration = frames / float(rate)\r\n",
        "    print(duration)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUO6r4eqtDmm",
        "outputId": "01de806d-9963-4baf-b9e3-e0f71ffbb5cb"
      },
      "source": [
        "STEP =0.16 #training size step (more step equal more accurate standard 0.25)\r\n",
        "WINDOW_SIZE = duration # input segment length for NN we used for training 1.28\r\n",
        "# sample rate, Hz\r\n",
        "SAMPLE_RATE = 16000\r\n",
        "demo_wave ='a.wav'\r\n",
        "\r\n",
        "# Ground-truth is Yes No\r\n",
        "result = offline_inference(demo_wave, STEP, WINDOW_SIZE)\r\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[NeMo W 2021-01-13 12:45:13 nemo_logging:349] /usr/local/lib/python3.6/dist-packages/torch/functional.py:516: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
            "      normalized, onesided, return_complex)\n",
            "    \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "up\n",
            "up\n",
            "up\n",
            "sheila\n",
            "eight\n",
            "up\n",
            "up\n",
            "up\n",
            "up\n",
            "up\n",
            "up\n",
            "up\n",
            "up\n",
            "six\n",
            "up\n",
            "up\n",
            "up\n",
            "up\n",
            "eight\n",
            "nine\n",
            "up\n",
            "eight\n",
            "six\n",
            "off\n",
            "zero\n",
            "up\n",
            "off\n",
            "off\n",
            "nine\n",
            "nine\n",
            "off\n",
            "off\n",
            "off\n",
            "left\n",
            "eight\n",
            "two\n",
            "off\n",
            "up\n",
            "up\n",
            "marvin\n",
            "eight\n",
            "two\n",
            "off\n",
            "off\n",
            "off\n",
            "off\n",
            "off\n",
            "on\n",
            "two\n",
            "off\n",
            "no\n",
            "off\n",
            "on\n",
            "three\n",
            "on\n",
            "eight\n",
            "off\n",
            "off\n",
            "left\n",
            "off\n",
            "no\n",
            "zero\n",
            "off\n",
            "no\n",
            "two\n",
            "eight\n",
            "no\n",
            "left\n",
            "eight\n",
            "one\n",
            "on\n",
            "left\n",
            "eight\n",
            "no\n",
            "one\n",
            "off\n",
            "up\n",
            "eight\n",
            "eight\n",
            "left\n",
            "left\n",
            "off\n",
            "eight\n",
            "off\n",
            "on\n",
            "one\n",
            "on\n",
            "off\n",
            "eight\n",
            "eight\n",
            "up\n",
            "on\n",
            "off\n",
            "eight\n",
            "eight\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC_CEFeLxAxk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "19664dd4-56ac-4799-b614-392be3e7b8d5"
      },
      "source": [
        "\r\n",
        "IPython.display.Audio(text_to_audio(result), rate = 27000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "                <audio controls=\"controls\" >\n",
              "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}